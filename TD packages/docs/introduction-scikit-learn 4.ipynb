{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_disease = pd.read_csv(\"../data/heart-disease.csv\")\n",
    "heart_disease.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "194",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7q/5sn_ssd501gcz8xq_xyjv7gm0000gn/T/ipykernel_1069/678152420.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Obtenez l'ensemble de données sur le logement en Californie\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfetch_california_housing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhousing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_california_housing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mhousing\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0;31m# est téléchargé sous forme de dictionnaire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/machine_learning_course/venv/lib/python3.8/site-packages/sklearn/datasets/_california_housing.py\u001b[0m in \u001b[0;36mfetch_california_housing\u001b[0;34m(data_home, download_if_missing, return_X_y, as_frame)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mcal_housing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     feature_names = [\n",
      "\u001b[0;32m~/PycharmProjects/machine_learning_course/venv/lib/python3.8/site-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    585\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mload_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/machine_learning_course/venv/lib/python3.8/site-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36m_unpickle\u001b[0;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m             warnings.warn(\"The file '%s' has been generated with a \"\n",
      "\u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1210\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1212\u001b[0;31m                 \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1213\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 194"
     ]
    }
   ],
   "source": [
    "# Obtenez l'ensemble de données sur le logement en Californie\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()\n",
    "housing; # est téléchargé sous forme de dictionnaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df = pd.DataFrame(housing[\"data\"], columns=housing[\"feature_names\"])\n",
    "housing_df[\"target\"] = pd.Series(housing[\"target\"])\n",
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Évaluer un modèle\n",
    "\n",
    "Une fois que vous avez formé un modèle, vous aurez besoin d'un moyen de mesurer la fiabilité de ses prédictions.\n",
    "\n",
    "Dans l'ensemble, l'idée principale de l'évaluation d'un modèle est de **comparer les prédictions du modèle à ce qu'elles auraient idéalement dû être** (les étiquettes de vérité).\n",
    "\n",
    "Scikit-Learn implémente 3 méthodes différentes d'évaluation des modèles.\n",
    "\n",
    "1. La méthode `score()`. L'appel de `score()` sur une instance de modèle renverra une métrique associée au type de modèle que vous utilisez. La métrique dépend du modèle que vous utilisez.\n",
    "2. Le paramètre « scoring ». Ce paramètre peut être transmis à des méthodes telles que [`cross_val_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score) ou [` GridSearchCV()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) pour indiquer à Scikit-Learn d'utiliser un type spécifique de métrique de notation.\n",
    "3. Fonctions métriques spécifiques au problème disponibles dans [`sklearn.metrics`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics). De la même manière que le paramètre « scoring » peut être transmis à différentes fonctions de notation, Scikit-Learn les implémente en tant que fonctions autonomes.\n",
    "\n",
    "La fonction de notation que vous utiliserez dépendra également du problème sur lequel vous travaillez.\n",
    "\n",
    "Les problèmes de classification ont des mesures d'évaluation et des fonctions de notation différentes des problèmes de régression.\n",
    "\n",
    "Regardons quelques exemples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Évaluation générale du modèle avec `score()`\n",
    "\n",
    "Si nous réduisons le code de notre problème de classification précédent (construire un classificateur pour prédire si une personne souffre ou non d'une maladie cardiaque en fonction de son dossier médical).\n",
    "\n",
    "Nous pouvons voir la méthode `score()` entrer en jeu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importez la classe modèle RandomForestClassifier depuis le module ensemble\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Configurer une graine aléatoire\n",
    "np.random.seed(42)\n",
    "\n",
    "# Divisez les données en X (caractéristiques/données) et y (target/labels)\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "# Divisé en ensembles de train et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Instancier le modèle (sur l'ensemble d'entraînement)\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Appelez la méthode d'ajustement sur le modèle et transmettez-lui les données d'entraînement\n",
    "clf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois que le modèle a été ajusté sur les données d'entraînement (`X_train`, `y_train`), nous pouvons appeler la méthode `score()` dessus et évaluer notre modèle sur les données de test, des données que le modèle n'a jamais vues auparavant (` X_test`, `y_test`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier le score du modèle (sur l'ensemble de test)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaque modèle dans Scikit-Learn implémente une métrique par défaut pour « score() » qui convient au problème.\n",
    "\n",
    "Par exemple:\n",
    "* Les modèles de classificateur utilisent généralement [`metrics.accuracy_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score) comme « score » par défaut. ()` métrique.\n",
    "* Les modèles de régression utilisent généralement [`metrics.r2_score`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html) comme métrique `score()` par défaut.\n",
    "* Il existe bien d'autres [classifications](https://scikit-learn.org/stable/modules/classes.html#classification-metrics) et [régressions](https://scikit-learn.org/stable/modules/classes .html#regression-metrics) métriques spécifiques implémentées dans `sklearn.metrics`.\n",
    "\n",
    "Parce que `clf` est une instance de `RandomForestClassifier`, la méthode `score()` utilise la précision moyenne comme méthode de score.\n",
    "\n",
    "Vous pouvez le trouver en appuyant sur **SHIFT + TAB** (dans un bloc-notes Jupyter, peut être différent ailleurs) entre parenthèses de `score()` lorsqu'il est appelé sur une instance de modèle.\n",
    "\n",
    "En coulisses, `score()` fait des prédictions sur `X_test` à l'aide du modèle entraîné, puis compare ces prédictions aux étiquettes réelles `y_test`.\n",
    "\n",
    "Un modèle de classification qui prédit tout ce qui est correct à 100 % recevrait un score de précision de 1,0 (ou 100 %).\n",
    "\n",
    "Notre modèle ne donne pas tout ce qu'il faut, mais avec une précision d'environ 85 % (0,85 * 100), c'est toujours bien mieux que de deviner.\n",
    "\n",
    "Faisons de même mais avec le code de régression ci-dessus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importez la classe modèle RandomForestRegressor depuis le module ensemble\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Configurer une graine aléatoire\n",
    "np.random.seed(42)\n",
    "\n",
    "# Divisez les données en fonctionnalités (X) et étiquettes (y)\n",
    "X = housing_df.drop(\"target\", axis=1)\n",
    "y = housing_df[\"target\"]\n",
    "\n",
    "# Divisé en ensembles de train et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Instaurer et ajuster le modèle (sur l'ensemble d'entraînement)\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En raison de la conception cohérente de la bibliothèque Scikit-Learn, nous pouvons appeler la même méthode `score()` sur `model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier le score du modèle (sur l'ensemble de test)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, `model` est une instance de [`RandomForestRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html).\n",
    "\n",
    "Et comme il s'agit d'un modèle de régression, la métrique par défaut intégrée à `score()` est le coefficient de détermination ou R^2 (prononcé R-sqaured).\n",
    "\n",
    "N'oubliez pas que vous pouvez le trouver en appuyant sur **SHIFT + TAB** entre parenthèses de `score()` lorsqu'il est appelé sur une instance de modèle.\n",
    "\n",
    "La meilleure valeur possible ici est 1,0, cela signifie que le modèle prédit exactement les valeurs de régression cibles.\n",
    "\n",
    "Appeler la méthode `score()` sur n'importe quelle instance de modèle et lui transmettre des données de test est un bon moyen rapide de voir comment évolue votre modèle.\n",
    "\n",
    "Cependant, lorsque vous approfondissez un problème, vous souhaiterez probablement commencer à utiliser des métriques plus puissantes pour évaluer les performances de vos modèles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Évaluer vos modèles à l'aide du paramètre `scoring`\n",
    "\n",
    "La prochaine étape après l'utilisation de `score()` consiste à utiliser un paramètre `scoring` personnalisé avec [`cross_val_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score .html#sklearn.model_selection.cross_val_score) ou [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n",
    "\n",
    "Comme vous l'avez peut-être deviné, le paramètre « score » que vous définissez sera différent en fonction du problème sur lequel vous travaillez.\n",
    "\n",
    "Nous verrons quelques exemples spécifiques de différents paramètres dans un instant, mais regardons d'abord `cross_val_score()`.\n",
    "\n",
    "Pour ce faire, nous copierons le code de classification des maladies cardiaques ci-dessus, puis ajouterons une autre ligne en haut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importez cross_val_score depuis le module model_selection\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Importez la classe modèle RandomForestClassifier depuis le module ensemble\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Configurer une graine aléatoire\n",
    "np.random.seed(42)\n",
    "\n",
    "# Divisez les données en X (caractéristiques/données) et y (target/labels)\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "# Divisé en ensembles de train et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Instancier le modèle (sur l'ensemble d'entraînement)\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Appelez la méthode d'ajustement sur le modèle et transmettez-lui les données d'entraînement\n",
    "clf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'utilisation de `cross_val_score()` est légèrement différente de `score()`.\n",
    "\n",
    "Voyons d'abord un exemple de code, puis nous passerons en revue les détails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisant score()\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisant cross_val_score()\n",
    "cross_val_score(clf, X, y, cv=5) # cv = number of splits to test (5 by default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qu'est-ce qu'il se passe ici?\n",
    "\n",
    "La première différence que vous remarquerez peut-être est que `cross_val_score()` renvoie un tableau alors que `score()` ne renvoie qu'un seul nombre.\n",
    "\n",
    "`cross_val_score()` renvoie un tableau grâce à un paramètre appelé `cv`, qui signifie validation croisée.\n",
    "\n",
    "Lorsque `cv` n'est pas défini, `cross_val_score()` renverra un tableau de 5 nombres par défaut (`cv=None` équivaut à définir `cv=5`).\n",
    "\n",
    "N'oubliez pas que vous pouvez voir les paramètres d'une fonction en utilisant **SHIFT + TAB** (à l'intérieur d'un bloc-notes Jupyter) entre parenthèses.\n",
    "\n",
    "Mais attendez, vous vous demandez peut-être ce qu'est la validation croisée ?\n",
    "\n",
    "Un visuel pourrait peut-être aider.\n",
    "\n",
    "<img src='../docs/images/sklearn-cross-validation.png' width=600/>\n",
    "\n",
    "Nous avons traité la figure 1.0 avant d'utiliser `score(X_test, y_test)`.\n",
    "\n",
    "Mais en y regardant de plus près, si un modèle est entraîné à l'aide des données d'entraînement ou de 80 % des échantillons, cela signifie que 20 % des échantillons ne sont pas utilisés pour que le modèle apprenne quoi que ce soit.\n",
    "\n",
    "Cela signifie également qu'en fonction des 80 % utilisés pour l'entraînement et des 20 % utilisés pour évaluer le modèle, celui-ci peut obtenir un score qui ne reflète pas l'ensemble des données.\n",
    "\n",
    "Par exemple, si de nombreux exemples simples se trouvent dans les données d'entraînement à 80 %, lorsqu'il s'agit de tester sur les 20 %, votre modèle risque de mal fonctionner.\n",
    "\n",
    "Il en va de même pour l'inverse.\n",
    "\n",
    "La figure 2.0 montre la validation croisée 5 fois, une méthode qui tente de fournir une solution à :\n",
    "\n",
    "1. Ne pas former sur toutes les données (en gardant toujours les ensembles de formation et de test séparés).\n",
    "2. Éviter d'obtenir des scores chanceux sur des fractionnements uniques des données.\n",
    "\n",
    "Au lieu de s'entraîner uniquement sur 1 fractionnement de formation et d'évaluer sur 1 fractionnement de test, la validation croisée 5 fois le fait 5 fois.\n",
    "\n",
    "Sur un partage différent à chaque fois, renvoyant un score pour chacun.\n",
    "\n",
    "Pourquoi 5 fois ?\n",
    "\n",
    "Le nom réel de cette configuration de validation croisée K-fold. Où K est un nombre arbitraire. Nous avons utilisé 5 car il est joli visuellement et c'est la valeur par défaut dans [`sklearn.model_selection.cross_val_score`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score. html).\n",
    "\n",
    "La figure 2.0 montre ce qui se passe lorsque nous exécutons ce qui suit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross-validation\n",
    "cross_val_score(clf, X, y, cv=5) # cv is equivalent to K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puisque nous définissons `cv=5` (validation croisée 5 fois), nous récupérons 5 scores différents au lieu de 1.\n",
    "\n",
    "Prendre la moyenne de ce tableau nous donne une idée plus approfondie des performances de notre modèle en convertissant les 5 scores en un seul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Formation unique et score partagé de test\n",
    "clf_single_score = clf.score(X_test, y_test)\n",
    "\n",
    "# Prenez la moyenne 5-fold cross-validation\n",
    "clf_cross_val_score = np.mean(cross_val_score(clf, X, y, cv=5))\n",
    "\n",
    "clf_single_score, clf_cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarquez que la moyenne `cross_val_score()` est légèrement inférieure à la valeur unique renvoyée par `score()`.\n",
    "\n",
    "Dans ce cas, si on vous demandait de signaler la précision de votre modèle, même si elle est inférieure, vous préféreriez la métrique à validation croisée à la métrique non validée de manière croisée.\n",
    "\n",
    "Attendez?\n",
    "\n",
    "Nous n'avons pas du tout utilisé le paramètre `scoring`.\n",
    "\n",
    "Par défaut, il est défini sur « Aucun »."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(clf, X, y, cv=5, scoring=None) # valeur de notation par défaut, elle peut être définie sur d'autres mesures de notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Remarque :** Si vous remarquez des scores différents à chaque fois que vous appelez `cross_val_score`, c'est parce que chaque répartition des données est aléatoire à chaque fois. Ainsi, le modèle peut obtenir des scores plus élevés/inférieurs sur différentes divisions des données. Pour obtenir des scores reproductibles, vous pouvez définir la valeur de départ aléatoire.\n",
    "\n",
    "Lorsque `scoring` est défini sur `None` (par défaut), il utilise la même métrique que `score()` quel que soit le modèle transmis à `cross_val_score()`.\n",
    "\n",
    "Dans ce cas, notre modèle est `clf` qui est une instance de `RandomForestClassifier` qui utilise la précision moyenne comme métrique `score()` par défaut.\n",
    "\n",
    "Vous pouvez modifier le score d'évaluation utilisé par `cross_val_score()` en modifiant le paramètre `scoring`.\n",
    "\n",
    "Et comme vous l’avez peut-être deviné, différents problèmes nécessitent des scores d’évaluation différents.\n",
    "\n",
    "La [documentation Scikit-Learn](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter) décrit une vaste gamme de mesures d'évaluation pour différents problèmes, mais examinons-en quelques-unes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Métriques d'évaluation du modèle de classification\n",
    "\n",
    "Quatre des principales mesures/méthodes d'évaluation que vous rencontrerez pour les modèles de classification sont :\n",
    "\n",
    "1. [Précision](https://developers.google.com/machine-learning/crash-course/classification/accuracy)\n",
    "2. [Zone sous la courbe ROC](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc) (courbe caractéristique de fonctionnement du récepteur)\n",
    "3. [Matrice de confusion](https://en.wikipedia.org/wiki/Confusion_matrix)\n",
    "4. [Rapport de classification](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)\n",
    "\n",
    "Jetons un coup d'oeil à chacun d'eux. Nous allons descendre le code de classification ci-dessus pour passer en revue quelques exemples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importez cross_val_score depuis le module model_selection\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Précision\n",
    "La précision est la métrique par défaut pour la fonction `score()` dans chacun des modèles de classificateur de Scikit-Learn. Et c’est probablement la métrique que vous verrez le plus souvent utilisée pour les problèmes de classification.\n",
    "\n",
    "Cependant, nous verrons dans une seconde que ce n’est pas toujours la meilleure métrique à utiliser.\n",
    "\n",
    "Scikit-Learn renvoie la précision sous forme décimale, mais vous pouvez facilement la convertir en pourcentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Précision/Accuracy en pourcentage\n",
    "print(f\"Heart Disease Classifier Accuracy: {clf.score(X_test, y_test) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Surface sous la courbe des caractéristiques de fonctionnement du récepteur (ROC)\n",
    "Si celui-ci ressemble à une bouchée, c’est parce que lire le nom complet l’est.\n",
    "\n",
    "On l'appelle généralement AUC pour Area Under Curve et la courbe dont ils parlent est la caractéristique de fonctionnement du récepteur ou ROC en abrégé.\n",
    "\n",
    "Donc, si vous entendez quelqu'un parler d'AUC ou de ROC, il parle probablement de ce qui suit.\n",
    "\n",
    "Les courbes ROC sont une comparaison du taux de vrais positifs (tpr) par rapport au taux de faux positifs (fpr).\n",
    "\n",
    "Pour plus de clarté:\n",
    "* Vrai positif = le modèle prédit 1 lorsque la vérité est 1\n",
    "* Faux positif = le modèle prédit 1 alors que la vérité est 0\n",
    "* Vrai négatif = le modèle prédit 0 lorsque la vérité est 0\n",
    "* Faux négatif = le modèle prédit 0 lorsque la vérité est 1\n",
    "\n",
    "Maintenant que nous le savons, voyons-en un. Scikit-Learn vous permet de calculer les informations requises pour une courbe ROC à l'aide du [`roc_curve`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve ) fonction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Faites des prédictions avec des probabilités\n",
    "y_probs = clf.predict_proba(X_test)\n",
    "\n",
    "# Conserver uniquement les probabilités de la classe positive\n",
    "y_probs = y_probs[:, 1]\n",
    "\n",
    "# Calculer fpr, tpr et seuils\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
    "\n",
    "# Vérifiez le taux de faux positifs\n",
    "fpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les regarder seuls n’a pas beaucoup de sens. Il est beaucoup plus facile de voir leur valeur visuellement.\n",
    "\n",
    "Créons une fonction d'assistance pour créer une courbe ROC étant donné les taux de faux positifs (`fpr`) et de vrais positifs (`tpr`).\n",
    "\n",
    "> **Remarque :** Depuis Scikit-Learn 1.2+, il existe une fonctionnalité permettant de tracer une courbe ROC. Vous pouvez le trouver sous [`sklearn.metrics.RocCurveDisplay`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.RocCurveDisplay.html#sklearn-metrics-roccurvedisplay)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_roc_curve(fpr, tpr):\n",
    "    \"\"\"\n",
    "    Plots a ROC curve given the false positve rate (fpr) and \n",
    "    true postive rate (tpr) of a classifier.\n",
    "    \"\"\"\n",
    "    # Tracé la courbe ROC\n",
    "    plt.plot(fpr, tpr, color='orange', label='ROC')\n",
    "    # Ligne de tracé sans pouvoir prédictif (référence)\n",
    "    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--', label='Guessing')\n",
    "    # Customisez le graph\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_roc_curve(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En regardant l’intrigue pour la première fois, cela peut paraître un peu déroutant.\n",
    "\n",
    "La principale chose à retenir ici est que notre modèle fait bien mieux que deviner.\n",
    "\n",
    "Une métrique que vous pouvez utiliser pour quantifier la courbe ROC en un seul nombre est l'AUC (Area Under Curve).\n",
    "\n",
    "Scikit-Learn implémente une fonction pour calculer cela appelée [`sklearn.metrics.roc_auc_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score) .\n",
    "\n",
    "Le score ROC AUC maximum que vous pouvez atteindre est de 1,0 et généralement, plus il est proche de 1,0, meilleur est le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score_value = roc_auc_score(y_test, y_probs)\n",
    "roc_auc_score_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien qu'il s'agisse d'une bonne pratique, nous n'avons pas réellement eu besoin de créer notre propre fonction `plot_roc_curve`.\n",
    "\n",
    "Scikit-Learn nous permet de tracer une courbe ROC directement à partir de notre estimateur/modèle en utilisant la méthode de classe [`sklearn.metrics.RocCurveDisplay.from_estimator`](https://scikit-learn.org/stable/modules/generated/sklearn .metrics.RocCurveDisplay.html#sklearn.metrics.RocCurveDisplay.from_estimator) et en lui transmettant nos `estimateur`, `X_test` et `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "roc_curve_display = RocCurveDisplay.from_estimator(estimator=clf, \n",
    "                                                   X=X_test, \n",
    "                                                   y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La position la plus idéale pour qu’une courbe ROC s’étende le long du coin supérieur gauche du tracé.\n",
    "\n",
    "Cela signifierait que le modèle prédit uniquement les vrais positifs et aucun faux positif. Et cela donnerait un score ROC AUC de 1,0.\n",
    "\n",
    "Vous pouvez le voir en créant une courbe ROC en utilisant uniquement les étiquettes « y_test »."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracé une courbe ROC parfaite\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test)\n",
    "plot_roc_curve(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score ROC AUC parfait\n",
    "roc_auc_score(y_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrice de confusion\n",
    "\n",
    "Une autre façon fantastique d'évaluer un modèle de classification consiste à utiliser une [matrice de confusion](https://en.wikipedia.org/wiki/Confusion_matrix).\n",
    "\n",
    "Une matrice de confusion est un moyen rapide de comparer les étiquettes prédites par un modèle et les étiquettes réelles qu’il était censé prédire.\n",
    "\n",
    "Essentiellement, cela vous donne une idée de la confusion dans le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "confusion_matrix(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encore une fois, c'est probablement plus facile à visualiser.\n",
    "\n",
    "Une façon de le faire est d'utiliser `pd.crosstab()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(y_test, \n",
    "            y_preds, \n",
    "            rownames=[\"Actual Label\"], \n",
    "            colnames=[\"Predicted Label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création d'une matrice de confusion à l'aide de Scikit-Learn\n",
    "\n",
    "Scikit-Learn a plusieurs implémentations différentes de tracé de matrices de confusion :\n",
    "\n",
    "1. [`sklearn.metrics.ConfusionMatrixDisplay.from_estimator(estimator, X, y)`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay .from_estimator) - cela prend un estimateur ajusté (comme notre modèle `clf`), des caractéristiques (`X`) et des étiquettes (`y`), il utilise ensuite l'estimateur entraîné pour faire des prédictions sur `X` et compare les prédictions à `y` en affichant une matrice de confusion.\n",
    "2. [`sklearn.metrics.ConfusionMatrixDisplay.from_predictions(y_true, y_pred)`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay.from_predictions ) - cela prend les étiquettes de vérité et les étiquettes prédites et les compare en affichant une matrice de confusion.\n",
    "\n",
    "> **Remarque :** Ces deux méthodes/classes nécessitent Scikit-Learn 1.0+. Pour vérifier votre version de Scikit-Learn, exécutez :\n",
    "```python\n",
    "importer Sklearn\n",
    "sklearn.__version__\n",
    "```\n",
    "> Si vous n'avez pas la version 1.0+, vous pouvez effectuer la mise à niveau sur : https://scikit-learn.org/stable/install.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(estimator=clf, X=X, y=y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracez la matrice de confusion à partir des prédictions\n",
    "ConfusionMatrixDisplay.from_predictions(y_true=y_test, \n",
    "                                        y_pred=y_preds);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rapport de classement\n",
    "\n",
    "La dernière mesure majeure à prendre en compte lors de l'évaluation d'un modèle de classification est un rapport de classification.\n",
    "\n",
    "Un rapport de classification est davantage un ensemble de mesures qu’une seule.\n",
    "\n",
    "Vous pouvez créer un rapport de classification à l'aide de la méthode [sklearn.metrics.classification_report`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) de Scikit-Learn.\n",
    "\n",
    "Voyons-en un."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il renvoie quatre colonnes : précision, rappel, score f1 et support.\n",
    "\n",
    "Le nombre de lignes dépend du nombre de classes différentes. Mais il y aura toujours trois lignes de précision d'étiquette, de moyenne macro et de moyenne pondérée.\n",
    "\n",
    "Chaque terme mesure quelque chose de légèrement différent :\n",
    "* **Précision** - Indique la proportion d'identifications positives (classe « 1 prédite par le modèle) qui étaient réellement correctes. Un modèle qui ne produit aucun faux positif a une précision de 1,0.\n",
    "* **Rappel** - Indique la proportion de positifs réels qui ont été correctement classés. Un modèle qui ne produit aucun faux négatif a un rappel de 1,0.\n",
    "* **Score F1** - Une combinaison de précision et de rappel. Un modèle parfait obtient un score F1 de 1,0.\n",
    "* **Support** - Le nombre d'échantillons sur lesquels chaque métrique a été calculée.\n",
    "* **Précision** - La précision du modèle sous forme décimale. La précision parfaite est égale à 1,0, ce qui signifie que la prédiction est correcte dans 100 % des cas.\n",
    "* **Macro avg** - Abréviation de macro moyenne, la précision moyenne, le rappel et le score F1 entre les classes. La macro moyenne ne prend pas en compte le déséquilibre des classes. Donc, si vous avez des déséquilibres de classe (plus d’exemples d’une classe que d’une autre), vous devez y prêter attention.\n",
    "* **Moyenne pondérée** - Abréviation de moyenne pondérée, la précision moyenne pondérée, le rappel et le score F1 entre les classes. Pondéré signifie que chaque métrique est calculée en fonction du nombre d'échantillons qu'il y a dans chaque classe. Cette métrique favorisera la classe majoritaire (par exemple, elle donnera une valeur élevée lorsqu'une classe en surpassera une autre en raison du plus grand nombre d'échantillons).\n",
    "\n",
    "Quand devriez-vous utiliser chacun d’eux ?\n",
    "\n",
    "Il peut être tentant de baser les performances de vos modèles de classification uniquement sur la précision. Et la précision est une bonne mesure à signaler, sauf lorsque vous avez des classes très déséquilibrées.\n",
    "\n",
    "Par exemple, disons qu'il y avait 10 000 personnes. Et 1 d’entre eux avait une maladie. Il vous est demandé de construire un modèle pour prédire qui en est atteint.\n",
    "\n",
    "Vous construisez le modèle et constatez que votre modèle est précis à 99,99 %. Ce qui semble génial !\n",
    "...jusqu'à ce que vous vous en rendiez compte, tout ce qu'il fait, c'est prédire que personne ne sera atteint de la maladie, en d'autres termes, les 10 000 prédictions sont fausses.\n",
    "\n",
    "Dans ce cas, vous voudriez vous tourner vers des mesures telles que la précision, le rappel et le score F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Où la précision et le rappel deviennent précieux\n",
    "disease_true = np.zeros(10000)\n",
    "disease_true[0] = 1 # seulement 1 cas\n",
    "\n",
    "disease_preds = np.zeros(10000) # toute les prédiction sont 0\n",
    "pd.DataFrame(classification_report(disease_true, \n",
    "                                   disease_preds, \n",
    "                                   output_dict=True,\n",
    "                                   zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez voir ici, nous avons une précision de 0,9999 (99,99%), une grande précision et rappel sur la classe 0,0 mais rien pour la classe 1,0.\n",
    "\n",
    "Demandez-vous : même si le modèle atteint une précision de 99,99 %, est-il utile ?\n",
    "\n",
    "Résumer:\n",
    "* La précision est une bonne mesure pour commencer si toutes les classes sont équilibrées (par exemple, la même quantité d'échantillons étiquetés avec 0 ou 1)\n",
    "* La précision et le rappel deviennent plus importants lorsque les cours sont déséquilibrés.\n",
    "* Si les prédictions faussement positives sont pires que les fausses négatives, visez une plus grande précision.\n",
    "* Si les prédictions faussement négatives sont pires que les fausses positives, visez un rappel plus élevé.\n",
    "\n",
    "> **Ressource :** Pour en savoir plus sur la précision, le rappel et les compromis entre eux, je suggère de consulter le [guide Scikit-Learn Precision-Recall](https://scikit-learn.org/stable/auto_examples/ model_selection/plot_precision_recall.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Métriques d'évaluation du modèle de régression\n",
    "\n",
    "Semblable à la classification, il existe [plusieurs métriques que vous pouvez utiliser pour évaluer vos modèles de régression](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics).\n",
    "\n",
    "Nous allons vérifier ce qui suit.\n",
    "\n",
    "1. **R^2 (prononcé r-carré) ou coefficient de détermination** - Compare les prédictions de vos modèles à la moyenne des cibles. Les valeurs peuvent aller de l'infini négatif (un modèle très médiocre) à 1. Par exemple, si votre modèle ne fait que prédire la moyenne des cibles, sa valeur R^2 serait de 0. Et si votre modèle prédit parfaitement une plage de nombres sa valeur R ^ 2 serait de 1. Plus c'est élevé, mieux c'est.\n",
    "2. **Erreur absolue moyenne (MAE)** - La moyenne des différences absolues entre les prédictions et les valeurs réelles. Cela vous donne une idée de l’erreur de vos prédictions. Plus bas, c'est mieux.\n",
    "3. **Erreur quadratique moyenne (MSE)** - Les différences quadratiques moyennes entre les prédictions et les valeurs réelles. La mise au carré des erreurs supprime les erreurs négatives. Cela amplifie également les valeurs aberrantes (échantillons qui comportent des erreurs plus importantes). Plus bas, c'est mieux.\n",
    "\n",
    "Voyons-les en action. Tout d’abord, nous allons à nouveau supprimer notre code de modèle de régression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importez la classe modèle RandomForestRegressor depuis le module ensemble\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Configurer une graine aléatoire\n",
    "np.random.seed(42)\n",
    "\n",
    "# Diviser les données en fonctionnalités (X) et étiquettes (y)\n",
    "X = housing_df.drop(\"target\", axis=1)\n",
    "y = housing_df[\"target\"]\n",
    "\n",
    "# Divisé en ensembles de train et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Instaurer et ajuster le modèle (sur l'ensemble d'entraînement)\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Score R^2 (coefficient de détermination)**\n",
    "\n",
    "Une fois que vous disposez d'un modèle de régression entraîné, la métrique d'évaluation par défaut dans la fonction `score()` est R^2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer le score R^2 du modèle\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En dehors de la fonction `score()`, R^2 peut être calculé à l'aide de [`r2_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score. html#sklearn.metrics.r2_score).\n",
    "\n",
    "Un modèle qui prédit uniquement la moyenne obtiendrait un score de 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Remplir un tableau avec la moyenne y_test\n",
    "y_test_mean = np.full(len(y_test), y_test.mean())\n",
    "\n",
    "r2_score(y_test, y_test_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et un modèle parfait obtiendrait une note de 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour vos modèles de régression, vous souhaiterez maximiser R ^ 2, tout en minimisant MAE et MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Erreur absolue moyenne (MAE)**\n",
    "\n",
    "L'erreur absolue moyenne d'un modèle peut être calculée avec la méthode [`sklearn.metrics.mean_absolute_error`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html) de Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erreur absolue moyenne\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "y_preds = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_preds)\n",
    "mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre modèle atteint un MAE de 0,327.\n",
    "\n",
    "Cela signifie qu'en moyenne, les prévisions de nos modèles sont éloignées de 0,327 unité de la valeur réelle.\n",
    "\n",
    "Rendons cela un peu plus visuel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={\"actual values\": y_test, \n",
    "                   \"predictions\": y_preds})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez voir que les prévisions sont légèrement différentes des valeurs réelles.\n",
    "\n",
    "Selon le problème sur lequel vous travaillez, avoir une différence comme nous le faisons actuellement peut être acceptable. D’un autre côté, cela pourrait ne pas aller non plus, ce qui signifie que les prédictions devraient être plus proches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x = np.arange(0, len(df), 1)\n",
    "ax.scatter(x, df[\"actual values\"], c='b', label=\"Acutual Values\")\n",
    "ax.scatter(x, df[\"predictions\"], c='r', label=\"Predictions\")\n",
    "ax.legend(loc=(1, 0.5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Erreur quadratique moyenne (MSE)**\n",
    "\n",
    "Et le MSE ?\n",
    "\n",
    "Nous pouvons le calculer avec [`sklearn.metrics.mean_squared_error`] de Scikit-Learn(https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erreur quadratique moyenne\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(y_test, y_preds)\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le MSE sera souvent supérieur au MAE car il correspond au carré des erreurs plutôt que de prendre uniquement en compte la différence absolue.\n",
    "\n",
    "Maintenant, vous vous demandez peut-être quelle métrique d’évaluation de régression devriez-vous utiliser ?\n",
    "\n",
    "* R^2 est similaire à la précision. Cela vous donne une indication rapide des performances possibles de votre modèle. Généralement, plus votre valeur R^2 est proche de 1,0, meilleur est le modèle. Mais cela ne dit pas vraiment à quel point votre modèle est erroné en termes d'écart avec chaque prédiction.\n",
    "* MAE donne une meilleure indication de l'écart moyen entre chacune des prédictions de votre modèle.\n",
    "* Quant au MAE ou au MSE, en raison de la façon dont le MSE est calculé, mettant au carré les différences entre les valeurs prédites et les valeurs réelles, il amplifie les différences plus importantes. Disons que nous prédisons la valeur des maisons (ce que nous faisons).\n",
    "     * Faites plus attention au MAE : lorsqu'une réduction de 10 000 $ est ***deux fois*** aussi mauvaise qu'une réduction de 5 000 $.\n",
    "     * Faites plus attention au MSE : lorsqu'une réduction de 10 000 $ est ***plus de deux fois*** aussi mauvaise qu'une réduction de 5 000 $.\n",
    "    \n",
    "> **Remarque :** Ce que nous avons couvert ici ne représente qu'une poignée de mesures potentielles que vous pouvez utiliser pour évaluer vos modèles. Si vous recherchez une liste complète, consultez la [documentation sur les métriques et les scores Scikit-Learn](https://scikit-learn.org/stable/modules/model_evaluation.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Évaluation d'un modèle à l'aide du paramètre `scoring`\n",
    "\n",
    "Nous avons abordé de nombreuses façons d'évaluer les prédictions d'un modèle, mais nous n'avons même pas touché au paramètre « score »...\n",
    "\n",
    "Ne vous inquiétez pas, c'est très similaire à ce que nous faisons !\n",
    "\n",
    "En guise d'actualisation, le paramètre `scoring` peut être utilisé avec une fonction telle que `cross_val_score()` pour indiquer à Scikit-Learn quelle métrique d'évaluation renvoyer en utilisant la validation croisée.\n",
    "\n",
    "Vérifions cela avec notre modèle de classification et l'ensemble de données sur les maladies cardiaques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout d’abord, nous utiliserons la valeur par défaut, qui correspond à la précision moyenne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "cv_acc = cross_val_score(clf, X, y, cv=5)\n",
    "cv_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons déjà vu cela auparavant, nous avons maintenant 5 scores de précision différents sur différentes fractions de test des données.\n",
    "\n",
    "La moyenne donne la précision validée croisée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validated accuracy\n",
    "print(f\"The cross-validated accuracy is: {np.mean(cv_acc)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons trouver la même chose en utilisant le paramètre `scoring` et en lui passant `\"précision\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "cv_acc = cross_val_score(clf, X, y, cv=5, scoring=\"accuracy\")\n",
    "print(f\"The cross-validated accuracy is: {np.mean(cv_acc)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il en va de même pour les autres mesures que nous utilisons pour la classification.\n",
    "\n",
    "Essayons la « précision »."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "cv_precision = cross_val_score(clf, X, y, cv=5, scoring=\"precision\")\n",
    "print(f\"The cross-validated precision is: {np.mean(cv_precision):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que diriez-vous du « rappel » ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "cv_recall = cross_val_score(clf, X, y, cv=5, scoring=\"recall\")\n",
    "print(f\"The cross-validated recall is: {np.mean(cv_recall):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et `\"f1\"` (pour score F1) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "cv_f1 = cross_val_score(clf, X, y, cv=5, scoring=\"f1\")\n",
    "print(f\"The cross-validated F1 score is: {np.mean(cv_f1):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons répéter ce processus avec nos métriques de régression.\n",
    "\n",
    "Revisitons notre modèle de régression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = housing_df.drop(\"target\", axis=1)\n",
    "y = housing_df[\"target\"]\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La valeur par défaut est `\"r2\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "cv_r2 = cross_val_score(model, X, y, cv=5, scoring=\"r2\")\n",
    "print(f\"The cross-validated R^2 score is: {np.mean(cv_r2):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais nous pouvons utiliser `\"neg_mean_absolute_error\"` pour MAE (erreur absolue moyenne)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "cv_mae = cross_val_score(model, X, y, cv=5, scoring=\"neg_mean_absolute_error\")\n",
    "print(f\"The cross-validated MAE score is: {np.mean(cv_mae):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pourquoi le « neg_ » ?\n",
    "\n",
    "Parce que la documentation Scikit-Learn indique :\n",
    "> [\"Tous les objets scorer suivent la convention selon laquelle des valeurs de retour plus élevées sont meilleures que des valeurs de retour plus faibles.\"](https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values)\n",
    "\n",
    "Ce qui dans ce cas signifie qu’une valeur négative inférieure (plus proche de 0) est préférable.\n",
    "\n",
    "Qu'en est-il de « neg_mean_squared_error » pour MSE (erreur quadratique moyenne) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "cv_mse = cross_val_score(model, \n",
    "                         X, \n",
    "                         y, \n",
    "                         cv=5,\n",
    "                         scoring=\"neg_mean_squared_error\")\n",
    "print(f\"The cross-validated MSE score is: {np.mean(cv_mse):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Utiliser différentes métriques d'évaluation avec Scikit-Learn\n",
    "\n",
    "Vous vous souvenez de la troisième façon d'évaluer les fonctions Scikit-Learn ?\n",
    "\n",
    "> 3. Fonctions métriques spécifiques au problème. De la même manière que le paramètre « scoring » peut être transmis à différentes fonctions de notation, Scikit-Learn les implémente en tant que fonctions autonomes.\n",
    "\n",
    "Eh bien, nous avons en quelque sorte abordé cette troisième façon d'utiliser les métriques d'évaluation avec Scikit-Learn.\n",
    "\n",
    "Essentiellement, toutes les métriques que nous avons vues précédemment ont leur propre fonction dans Scikit-Learn.\n",
    "\n",
    "Ils fonctionnent tous en comparant un tableau de prédictions, généralement appelé « y_preds » à un tableau d'étiquettes réelles, généralement appelé « y_test » ou « y_true »."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fonctions de classification\n",
    "Pour:\n",
    "* Précision que nous pouvons utiliser [`sklearn.metrics.accuracy_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n",
    "* Précision que nous pouvons utiliser [`sklearn.metrics.precision_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html)\n",
    "* Rappelez-vous que nous pouvons utiliser [`sklearn.metrics.recall_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)\n",
    "* F1, nous pouvons utiliser [`sklearn.metrics.f1_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Faites des prédictions\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# Evaluez le classificateur\n",
    "print(\"Classifier metrics on the test set:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_preds) * 100:.2f}%\")\n",
    "print(f\"Precision: {precision_score(y_test, y_preds):.2f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_preds):.2f}\")\n",
    "print(f\"F1: {f1_score(y_test, y_preds):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Métriques de régression\n",
    "\n",
    "Nous pouvons utiliser une configuration similaire pour notre problème de régression, mais avec des méthodes différentes.\n",
    "\n",
    "Pour:\n",
    "* R^2 nous pouvons utiliser [`sklearn.metrics.r2_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html)\n",
    "* MAE (erreur absolue moyenne) que nous pouvons utiliser [`sklearn.metrics.mean_absolute_error`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html)\n",
    "* MSE (erreur quadratique moyenne) que nous pouvons utiliser [`sklearn.metrics.mean_squared_error`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = housing_df.drop(\"target\", axis=1)\n",
    "y = housing_df[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.2)\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, \n",
    "                              n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Faites des prédictions\n",
    "y_preds = model.predict(X_test)\n",
    "\n",
    "# Evaluez le modèle\n",
    "print(\"Regression model metrics on the test set:\")\n",
    "print(f\"R^2: {r2_score(y_test, y_preds):.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_preds):.2f}\")\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_preds):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parce qu'évaluer les prédictions d'un modèle est aussi important que former un modèle dans tout projet d'apprentissage automatique.\n",
    "\n",
    "Il n'y a rien de pire que de former un modèle d'apprentissage automatique et de l'optimiser en fonction d'une mauvaise métrique d'évaluation.\n",
    "\n",
    "Conservez les métriques et les méthodes d'évaluation que nous avons utilisées lors de la formation de vos futurs modèles.\n",
    "\n",
    "Si vous recherchez des lectures supplémentaires, je consulterais le [guide Scikit-Learn pour l'évaluation des modèles](https://scikit-learn.org/stable/modules/model_evaluation.html).\n",
    "\n",
    "Maintenant que nous avons vu différentes métriques que nous pouvons utiliser pour évaluer un modèle, voyons comment nous pouvons améliorer ces métriques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "c556f6eb84b27a92005489cdcf9c9b80cc62ee891441f20eabfc5ad7282165a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
